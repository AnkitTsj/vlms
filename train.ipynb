{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T05:23:58.383468Z",
     "start_time": "2025-02-05T05:23:55.724343Z"
    }
   },
   "cell_type": "code",
   "source": "from model import *",
   "id": "112ae634eff62369",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T05:26:45.739700Z",
     "start_time": "2025-02-05T05:26:44.549344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "W_ESIZE = 124\n",
    "RNN_HIDDEN = 512\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "\n",
    "# Define Dataset & DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CocoCaptions(\n",
    "    root='D:/ankit/caption_data/train2017',\n",
    "    annFile='D:/ankit/caption_data/annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize Model\n",
    "model = Captioner(VSIZE, RNN_HIDDEN, BATCH_SIZE, W_ESIZE, 512, vocab, 80, rev_vocab,device).to(device)#.load_state_dict(torch.load(\"captioner_epoch_4.pth\",map_location=device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])  # Ignore padding in loss calculation\n"
   ],
   "id": "f01d8466b2cca0e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.70s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Ankit Kumar/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T10:02:35.094790Z",
     "start_time": "2025-02-05T05:26:46.803406Z"
    }
   },
   "source": [
    "# main training function call",
    "train(model,EPOCHS,data_loader,optimizer,criterion,10000)\n",
    "print(\"Training complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 100, Loss: 1.8450\n",
      "Epoch 1/10, Batch 200, Loss: 1.5866\n",
      "Epoch 1/10, Batch 300, Loss: 1.3585\n",
      "Epoch 1/10, Batch 400, Loss: 1.2689\n",
      "Epoch 1/10, Batch 500, Loss: 1.2608\n",
      "Epoch 1/10, Batch 600, Loss: 1.1715\n",
      "Epoch 1/10, Batch 700, Loss: 1.0953\n",
      "Epoch 1/10, Batch 800, Loss: 1.0726\n",
      "Epoch 1/10, Batch 900, Loss: 1.0047\n",
      "Epoch 1/10, Batch 1000, Loss: 1.0372\n",
      "Epoch 1/10, Batch 1100, Loss: 1.0053\n",
      "Epoch 1/10, Batch 1200, Loss: 0.8763\n",
      "Epoch 1/10, Batch 1300, Loss: 0.9858\n",
      "Epoch 1/10, Batch 1400, Loss: 0.9864\n",
      "Epoch 1/10, Batch 1500, Loss: 0.9162\n",
      "Epoch 1/10, Batch 1600, Loss: 0.8960\n",
      "Epoch 1/10, Batch 1700, Loss: 0.8911\n",
      "Epoch 1/10, Batch 1800, Loss: 0.9844\n",
      "Epoch 1/10, Batch 1900, Loss: 0.8802\n",
      "Epoch 1/10, Batch 2000, Loss: 0.9267\n",
      "Epoch 1/10, Batch 2100, Loss: 0.9417\n",
      "Epoch 1/10, Batch 2200, Loss: 0.8252\n",
      "Epoch 1/10, Batch 2300, Loss: 0.9693\n",
      "Epoch 1/10, Batch 2400, Loss: 0.8482\n",
      "Epoch 1/10, Batch 2500, Loss: 0.9199\n",
      "Epoch 1/10, Batch 2600, Loss: 0.9174\n",
      "Epoch 1/10, Batch 2700, Loss: 0.9395\n",
      "Epoch 1/10, Batch 2800, Loss: 0.8292\n",
      "Epoch 1/10, Batch 2900, Loss: 0.9225\n",
      "Epoch 1/10, Batch 3000, Loss: 0.8755\n",
      "Epoch 1/10, Batch 3100, Loss: 0.8846\n",
      "Epoch 1/10, Batch 3200, Loss: 0.7750\n",
      "Epoch 1/10, Batch 3300, Loss: 0.8861\n",
      "Epoch 1/10, Batch 3400, Loss: 0.8559\n",
      "Epoch 1/10, Batch 3500, Loss: 0.7784\n",
      "Epoch 1/10, Batch 3600, Loss: 0.7916\n",
      "Epoch 1/10, Average Loss: 1.0177\n",
      "Epoch 2/10, Batch 100, Loss: 0.7496\n",
      "Epoch 2/10, Batch 200, Loss: 0.7635\n",
      "Epoch 2/10, Batch 300, Loss: 0.7360\n",
      "Epoch 2/10, Batch 400, Loss: 0.7664\n",
      "Epoch 2/10, Batch 500, Loss: 0.7442\n",
      "Epoch 2/10, Batch 600, Loss: 0.7441\n",
      "Epoch 2/10, Batch 700, Loss: 0.8632\n",
      "Epoch 2/10, Batch 800, Loss: 0.7849\n",
      "Epoch 2/10, Batch 900, Loss: 0.8239\n",
      "Epoch 2/10, Batch 1000, Loss: 0.8126\n",
      "Epoch 2/10, Batch 1100, Loss: 0.8313\n",
      "Epoch 2/10, Batch 1200, Loss: 0.7456\n",
      "Epoch 2/10, Batch 1300, Loss: 0.7455\n",
      "Epoch 2/10, Batch 1400, Loss: 0.7473\n",
      "Epoch 2/10, Batch 1500, Loss: 0.6907\n",
      "Epoch 2/10, Batch 1600, Loss: 0.8249\n",
      "Epoch 2/10, Batch 1700, Loss: 0.7924\n",
      "Epoch 2/10, Batch 1800, Loss: 0.8768\n",
      "Epoch 2/10, Batch 1900, Loss: 0.8140\n",
      "Epoch 2/10, Batch 2000, Loss: 0.7362\n",
      "Epoch 2/10, Batch 2100, Loss: 0.7681\n",
      "Epoch 2/10, Batch 2200, Loss: 0.7146\n",
      "Epoch 2/10, Batch 2300, Loss: 0.7458\n",
      "Epoch 2/10, Batch 2400, Loss: 0.7493\n",
      "Epoch 2/10, Batch 2500, Loss: 0.7222\n",
      "Epoch 2/10, Batch 2600, Loss: 0.7115\n",
      "Epoch 2/10, Batch 2700, Loss: 0.7348\n",
      "Epoch 2/10, Batch 2800, Loss: 0.7909\n",
      "Epoch 2/10, Batch 2900, Loss: 0.7979\n",
      "Epoch 2/10, Batch 3000, Loss: 0.8641\n",
      "Epoch 2/10, Batch 3100, Loss: 0.8131\n",
      "Epoch 2/10, Batch 3200, Loss: 0.7088\n",
      "Epoch 2/10, Batch 3300, Loss: 0.7655\n",
      "Epoch 2/10, Batch 3400, Loss: 0.7986\n",
      "Epoch 2/10, Batch 3500, Loss: 0.6900\n",
      "Epoch 2/10, Batch 3600, Loss: 0.8110\n",
      "Epoch 2/10, Average Loss: 0.7747\n",
      "Epoch 3/10, Batch 100, Loss: 0.7348\n",
      "Epoch 3/10, Batch 200, Loss: 0.7894\n",
      "Epoch 3/10, Batch 300, Loss: 0.7270\n",
      "Epoch 3/10, Batch 400, Loss: 0.7191\n",
      "Epoch 3/10, Batch 500, Loss: 0.7522\n",
      "Epoch 3/10, Batch 600, Loss: 0.7362\n",
      "Epoch 3/10, Batch 700, Loss: 0.7181\n",
      "Epoch 3/10, Batch 800, Loss: 0.6997\n",
      "Epoch 3/10, Batch 900, Loss: 0.6580\n",
      "Epoch 3/10, Batch 1000, Loss: 0.6481\n",
      "Epoch 3/10, Batch 1100, Loss: 0.6322\n",
      "Epoch 3/10, Batch 1200, Loss: 0.8141\n",
      "Epoch 3/10, Batch 1300, Loss: 0.7783\n",
      "Epoch 3/10, Batch 1400, Loss: 0.7044\n",
      "Epoch 3/10, Batch 1500, Loss: 0.7962\n",
      "Epoch 3/10, Batch 1600, Loss: 0.6946\n",
      "Epoch 3/10, Batch 1700, Loss: 0.7159\n",
      "Epoch 3/10, Batch 1800, Loss: 0.8085\n",
      "Epoch 3/10, Batch 1900, Loss: 0.7063\n",
      "Epoch 3/10, Batch 2000, Loss: 0.6275\n",
      "Epoch 3/10, Batch 2100, Loss: 0.7263\n",
      "Epoch 3/10, Batch 2200, Loss: 0.6709\n",
      "Epoch 3/10, Batch 2300, Loss: 0.7330\n",
      "Epoch 3/10, Batch 2400, Loss: 0.7306\n",
      "Epoch 3/10, Batch 2500, Loss: 0.8489\n",
      "Epoch 3/10, Batch 2600, Loss: 0.7546\n",
      "Epoch 3/10, Batch 2700, Loss: 0.6264\n",
      "Epoch 3/10, Batch 2800, Loss: 0.8089\n",
      "Epoch 3/10, Batch 2900, Loss: 0.6411\n",
      "Epoch 3/10, Batch 3000, Loss: 0.6508\n",
      "Epoch 3/10, Batch 3100, Loss: 0.7927\n",
      "Epoch 3/10, Batch 3200, Loss: 0.6669\n",
      "Epoch 3/10, Batch 3300, Loss: 0.7210\n",
      "Epoch 3/10, Batch 3400, Loss: 0.8006\n",
      "Epoch 3/10, Batch 3500, Loss: 0.6273\n",
      "Epoch 3/10, Batch 3600, Loss: 0.6895\n",
      "Epoch 3/10, Average Loss: 0.7229\n",
      "Epoch 4/10, Batch 100, Loss: 0.7278\n",
      "Epoch 4/10, Batch 200, Loss: 0.6737\n",
      "Epoch 4/10, Batch 300, Loss: 0.6558\n",
      "Epoch 4/10, Batch 400, Loss: 0.6253\n",
      "Epoch 4/10, Batch 500, Loss: 0.7163\n",
      "Epoch 4/10, Batch 600, Loss: 0.7592\n",
      "Epoch 4/10, Batch 700, Loss: 0.6470\n",
      "Epoch 4/10, Batch 800, Loss: 0.6879\n",
      "Epoch 4/10, Batch 900, Loss: 0.6730\n",
      "Epoch 4/10, Batch 1000, Loss: 0.7112\n",
      "Epoch 4/10, Batch 1100, Loss: 0.6708\n",
      "Epoch 4/10, Batch 1200, Loss: 0.6803\n",
      "Epoch 4/10, Batch 1300, Loss: 0.6882\n",
      "Epoch 4/10, Batch 1400, Loss: 0.6003\n",
      "Epoch 4/10, Batch 1500, Loss: 0.7336\n",
      "Epoch 4/10, Batch 1600, Loss: 0.7280\n",
      "Epoch 4/10, Batch 1700, Loss: 0.7350\n",
      "Epoch 4/10, Batch 1800, Loss: 0.6223\n",
      "Epoch 4/10, Batch 1900, Loss: 0.6878\n",
      "Epoch 4/10, Batch 2000, Loss: 0.6516\n",
      "Epoch 4/10, Batch 2100, Loss: 0.7449\n",
      "Epoch 4/10, Batch 2200, Loss: 0.7076\n",
      "Epoch 4/10, Batch 2300, Loss: 0.6792\n",
      "Epoch 4/10, Batch 2400, Loss: 0.7720\n",
      "Epoch 4/10, Batch 2500, Loss: 0.7058\n",
      "Epoch 4/10, Batch 2600, Loss: 0.7529\n",
      "Epoch 4/10, Batch 2700, Loss: 0.7086\n",
      "Epoch 4/10, Batch 2800, Loss: 0.7413\n",
      "Epoch 4/10, Batch 2900, Loss: 0.7688\n",
      "Epoch 4/10, Batch 3000, Loss: 0.6010\n",
      "Epoch 4/10, Batch 3100, Loss: 0.6329\n",
      "Epoch 4/10, Batch 3200, Loss: 0.7097\n",
      "Epoch 4/10, Batch 3300, Loss: 0.7427\n",
      "Epoch 4/10, Batch 3400, Loss: 0.6228\n",
      "Epoch 4/10, Batch 3500, Loss: 0.6907\n",
      "Epoch 4/10, Batch 3600, Loss: 0.6844\n",
      "Epoch 4/10, Average Loss: 0.6949\n",
      "Epoch 5/10, Batch 100, Loss: 0.6986\n",
      "Epoch 5/10, Batch 200, Loss: 0.6840\n",
      "Epoch 5/10, Batch 300, Loss: 0.6637\n",
      "Epoch 5/10, Batch 400, Loss: 0.7149\n",
      "Epoch 5/10, Batch 500, Loss: 0.7007\n",
      "Epoch 5/10, Batch 600, Loss: 0.6871\n",
      "Epoch 5/10, Batch 700, Loss: 0.6759\n",
      "Epoch 5/10, Batch 800, Loss: 0.6884\n",
      "Epoch 5/10, Batch 900, Loss: 0.6962\n",
      "Epoch 5/10, Batch 1000, Loss: 0.7685\n",
      "Epoch 5/10, Batch 1100, Loss: 0.7012\n",
      "Epoch 5/10, Batch 1200, Loss: 0.6847\n",
      "Epoch 5/10, Batch 1300, Loss: 0.6229\n",
      "Epoch 5/10, Batch 1400, Loss: 0.6651\n",
      "Epoch 5/10, Batch 1500, Loss: 0.6037\n",
      "Epoch 5/10, Batch 1600, Loss: 0.6182\n",
      "Epoch 5/10, Batch 1700, Loss: 0.7030\n",
      "Epoch 5/10, Batch 1800, Loss: 0.6583\n",
      "Epoch 5/10, Batch 1900, Loss: 0.6400\n",
      "Epoch 5/10, Batch 2000, Loss: 0.6417\n",
      "Epoch 5/10, Batch 2100, Loss: 0.6818\n",
      "Epoch 5/10, Batch 2200, Loss: 0.6407\n",
      "Epoch 5/10, Batch 2300, Loss: 0.6562\n",
      "Epoch 5/10, Batch 2400, Loss: 0.6219\n",
      "Epoch 5/10, Batch 2500, Loss: 0.7577\n",
      "Epoch 5/10, Batch 2600, Loss: 0.7619\n",
      "Epoch 5/10, Batch 2700, Loss: 0.6508\n",
      "Epoch 5/10, Batch 2800, Loss: 0.6823\n",
      "Epoch 5/10, Batch 2900, Loss: 0.6189\n",
      "Epoch 5/10, Batch 3000, Loss: 0.6461\n",
      "Epoch 5/10, Batch 3100, Loss: 0.7222\n",
      "Epoch 5/10, Batch 3200, Loss: 0.6637\n",
      "Epoch 5/10, Batch 3300, Loss: 0.6903\n",
      "Epoch 5/10, Batch 3400, Loss: 0.7547\n",
      "Epoch 5/10, Batch 3500, Loss: 0.6082\n",
      "Epoch 5/10, Batch 3600, Loss: 0.6538\n",
      "Epoch 5/10, Average Loss: 0.6769\n",
      "Epoch 6/10, Batch 100, Loss: 0.6672\n",
      "Epoch 6/10, Batch 200, Loss: 0.6063\n",
      "Epoch 6/10, Batch 300, Loss: 0.6586\n",
      "Epoch 6/10, Batch 400, Loss: 0.7334\n",
      "Epoch 6/10, Batch 500, Loss: 0.6244\n",
      "Epoch 6/10, Batch 600, Loss: 0.5518\n",
      "Epoch 6/10, Batch 700, Loss: 0.7486\n",
      "Epoch 6/10, Batch 800, Loss: 0.6205\n",
      "Epoch 6/10, Batch 900, Loss: 0.6785\n",
      "Epoch 6/10, Batch 1000, Loss: 0.7200\n",
      "Epoch 6/10, Batch 1100, Loss: 0.6146\n",
      "Epoch 6/10, Batch 1200, Loss: 0.6271\n",
      "Epoch 6/10, Batch 1300, Loss: 0.7657\n",
      "Epoch 6/10, Batch 1400, Loss: 0.6663\n",
      "Epoch 6/10, Batch 1500, Loss: 0.6783\n",
      "Epoch 6/10, Batch 1600, Loss: 0.6755\n",
      "Epoch 6/10, Batch 1700, Loss: 0.7531\n",
      "Epoch 6/10, Batch 1800, Loss: 0.6163\n",
      "Epoch 6/10, Batch 1900, Loss: 0.6614\n",
      "Epoch 6/10, Batch 2000, Loss: 0.6568\n",
      "Epoch 6/10, Batch 2100, Loss: 0.6456\n",
      "Epoch 6/10, Batch 2200, Loss: 0.6182\n",
      "Epoch 6/10, Batch 2300, Loss: 0.6173\n",
      "Epoch 6/10, Batch 2400, Loss: 0.5898\n",
      "Epoch 6/10, Batch 2500, Loss: 0.7127\n",
      "Epoch 6/10, Batch 2600, Loss: 0.7030\n",
      "Epoch 6/10, Batch 2700, Loss: 0.6440\n",
      "Epoch 6/10, Batch 2800, Loss: 0.7371\n",
      "Epoch 6/10, Batch 2900, Loss: 0.7325\n",
      "Epoch 6/10, Batch 3000, Loss: 0.6357\n",
      "Epoch 6/10, Batch 3100, Loss: 0.6851\n",
      "Epoch 6/10, Batch 3200, Loss: 0.6216\n",
      "Epoch 6/10, Batch 3300, Loss: 0.6927\n",
      "Epoch 6/10, Batch 3400, Loss: 0.6733\n",
      "Epoch 6/10, Batch 3500, Loss: 0.6842\n",
      "Epoch 6/10, Batch 3600, Loss: 0.7395\n",
      "Epoch 6/10, Average Loss: 0.6623\n",
      "Epoch 7/10, Batch 100, Loss: 0.6341\n",
      "Epoch 7/10, Batch 200, Loss: 0.6528\n",
      "Epoch 7/10, Batch 300, Loss: 0.6636\n",
      "Epoch 7/10, Batch 400, Loss: 0.6638\n",
      "Epoch 7/10, Batch 500, Loss: 0.6600\n",
      "Epoch 7/10, Batch 600, Loss: 0.5917\n",
      "Epoch 7/10, Batch 700, Loss: 0.6464\n",
      "Epoch 7/10, Batch 800, Loss: 0.5794\n",
      "Epoch 7/10, Batch 900, Loss: 0.6512\n",
      "Epoch 7/10, Batch 1000, Loss: 0.5888\n",
      "Epoch 7/10, Batch 1100, Loss: 0.6984\n",
      "Epoch 7/10, Batch 1200, Loss: 0.6247\n",
      "Epoch 7/10, Batch 1300, Loss: 0.6698\n",
      "Epoch 7/10, Batch 1400, Loss: 0.6973\n",
      "Epoch 7/10, Batch 1500, Loss: 0.6150\n",
      "Epoch 7/10, Batch 1600, Loss: 0.6245\n",
      "Epoch 7/10, Batch 1700, Loss: 0.7458\n",
      "Epoch 7/10, Batch 1800, Loss: 0.6450\n",
      "Epoch 7/10, Batch 1900, Loss: 0.7545\n",
      "Epoch 7/10, Batch 2000, Loss: 0.7032\n",
      "Epoch 7/10, Batch 2100, Loss: 0.6435\n",
      "Epoch 7/10, Batch 2200, Loss: 0.6245\n",
      "Epoch 7/10, Batch 2300, Loss: 0.6421\n",
      "Epoch 7/10, Batch 2400, Loss: 0.7069\n",
      "Epoch 7/10, Batch 2500, Loss: 0.7039\n",
      "Epoch 7/10, Batch 2600, Loss: 0.6334\n",
      "Epoch 7/10, Batch 2700, Loss: 0.6512\n",
      "Epoch 7/10, Batch 2800, Loss: 0.6529\n",
      "Epoch 7/10, Batch 2900, Loss: 0.6646\n",
      "Epoch 7/10, Batch 3000, Loss: 0.6972\n",
      "Epoch 7/10, Batch 3100, Loss: 0.6991\n",
      "Epoch 7/10, Batch 3200, Loss: 0.5706\n",
      "Epoch 7/10, Batch 3300, Loss: 0.7478\n",
      "Epoch 7/10, Batch 3400, Loss: 0.5904\n",
      "Epoch 7/10, Batch 3500, Loss: 0.6297\n",
      "Epoch 7/10, Batch 3600, Loss: 0.6102\n",
      "Epoch 7/10, Average Loss: 0.6517\n",
      "Epoch 8/10, Batch 100, Loss: 0.5196\n",
      "Epoch 8/10, Batch 200, Loss: 0.6311\n",
      "Epoch 8/10, Batch 300, Loss: 0.5865\n",
      "Epoch 8/10, Batch 400, Loss: 0.6817\n",
      "Epoch 8/10, Batch 500, Loss: 0.5653\n",
      "Epoch 8/10, Batch 600, Loss: 0.6504\n",
      "Epoch 8/10, Batch 700, Loss: 0.6029\n",
      "Epoch 8/10, Batch 800, Loss: 0.6153\n",
      "Epoch 8/10, Batch 900, Loss: 0.7296\n",
      "Epoch 8/10, Batch 1000, Loss: 0.6766\n",
      "Epoch 8/10, Batch 1100, Loss: 0.6432\n",
      "Epoch 8/10, Batch 1200, Loss: 0.7151\n",
      "Epoch 8/10, Batch 1300, Loss: 0.6068\n",
      "Epoch 8/10, Batch 1400, Loss: 0.6689\n",
      "Epoch 8/10, Batch 1500, Loss: 0.6030\n",
      "Epoch 8/10, Batch 1600, Loss: 0.6326\n",
      "Epoch 8/10, Batch 1700, Loss: 0.6488\n",
      "Epoch 8/10, Batch 1800, Loss: 0.6345\n",
      "Epoch 8/10, Batch 1900, Loss: 0.6672\n",
      "Epoch 8/10, Batch 2000, Loss: 0.7180\n",
      "Epoch 8/10, Batch 2100, Loss: 0.6354\n",
      "Epoch 8/10, Batch 2200, Loss: 0.6538\n",
      "Epoch 8/10, Batch 2300, Loss: 0.6608\n",
      "Epoch 8/10, Batch 2400, Loss: 0.6222\n",
      "Epoch 8/10, Batch 2500, Loss: 0.6404\n",
      "Epoch 8/10, Batch 2600, Loss: 0.5943\n",
      "Epoch 8/10, Batch 2700, Loss: 0.6768\n",
      "Epoch 8/10, Batch 2800, Loss: 0.6364\n",
      "Epoch 8/10, Batch 2900, Loss: 0.7199\n",
      "Epoch 8/10, Batch 3000, Loss: 0.6564\n",
      "Epoch 8/10, Batch 3100, Loss: 0.6849\n",
      "Epoch 8/10, Batch 3200, Loss: 0.6577\n",
      "Epoch 8/10, Batch 3300, Loss: 0.7667\n",
      "Epoch 8/10, Batch 3400, Loss: 0.6057\n",
      "Epoch 8/10, Batch 3500, Loss: 0.6924\n",
      "Epoch 8/10, Batch 3600, Loss: 0.6294\n",
      "Epoch 8/10, Average Loss: 0.6431\n",
      "Epoch 9/10, Batch 100, Loss: 0.5871\n",
      "Epoch 9/10, Batch 200, Loss: 0.6431\n",
      "Epoch 9/10, Batch 300, Loss: 0.6048\n",
      "Epoch 9/10, Batch 400, Loss: 0.6066\n",
      "Epoch 9/10, Batch 500, Loss: 0.5946\n",
      "Epoch 9/10, Batch 600, Loss: 0.6814\n",
      "Epoch 9/10, Batch 700, Loss: 0.5758\n",
      "Epoch 9/10, Batch 800, Loss: 0.6148\n",
      "Epoch 9/10, Batch 900, Loss: 0.6451\n",
      "Epoch 9/10, Batch 1000, Loss: 0.5399\n",
      "Epoch 9/10, Batch 1100, Loss: 0.6812\n",
      "Epoch 9/10, Batch 1200, Loss: 0.6795\n",
      "Epoch 9/10, Batch 1300, Loss: 0.6238\n",
      "Epoch 9/10, Batch 1400, Loss: 0.5804\n",
      "Epoch 9/10, Batch 1500, Loss: 0.6142\n",
      "Epoch 9/10, Batch 1600, Loss: 0.5683\n",
      "Epoch 9/10, Batch 1700, Loss: 0.7077\n",
      "Epoch 9/10, Batch 1800, Loss: 0.7028\n",
      "Epoch 9/10, Batch 1900, Loss: 0.6959\n",
      "Epoch 9/10, Batch 2000, Loss: 0.6035\n",
      "Epoch 9/10, Batch 2100, Loss: 0.6193\n",
      "Epoch 9/10, Batch 2200, Loss: 0.5873\n",
      "Epoch 9/10, Batch 2300, Loss: 0.6814\n",
      "Epoch 9/10, Batch 2400, Loss: 0.5778\n",
      "Epoch 9/10, Batch 2500, Loss: 0.6772\n",
      "Epoch 9/10, Batch 2600, Loss: 0.6459\n",
      "Epoch 9/10, Batch 2700, Loss: 0.5976\n",
      "Epoch 9/10, Batch 2800, Loss: 0.6144\n",
      "Epoch 9/10, Batch 2900, Loss: 0.6526\n",
      "Epoch 9/10, Batch 3000, Loss: 0.5538\n",
      "Epoch 9/10, Batch 3100, Loss: 0.6138\n",
      "Epoch 9/10, Batch 3200, Loss: 0.6347\n",
      "Epoch 9/10, Batch 3300, Loss: 0.7002\n",
      "Epoch 9/10, Batch 3400, Loss: 0.6662\n",
      "Epoch 9/10, Batch 3500, Loss: 0.5922\n",
      "Epoch 9/10, Batch 3600, Loss: 0.6183\n",
      "Epoch 9/10, Average Loss: 0.6355\n",
      "Epoch 10/10, Batch 100, Loss: 0.6058\n",
      "Epoch 10/10, Batch 200, Loss: 0.6621\n",
      "Epoch 10/10, Batch 300, Loss: 0.6799\n",
      "Epoch 10/10, Batch 400, Loss: 0.6195\n",
      "Epoch 10/10, Batch 500, Loss: 0.6612\n",
      "Epoch 10/10, Batch 600, Loss: 0.6001\n",
      "Epoch 10/10, Batch 700, Loss: 0.5820\n",
      "Epoch 10/10, Batch 800, Loss: 0.6104\n",
      "Epoch 10/10, Batch 900, Loss: 0.5965\n",
      "Epoch 10/10, Batch 1000, Loss: 0.6115\n",
      "Epoch 10/10, Batch 1100, Loss: 0.6850\n",
      "Epoch 10/10, Batch 1200, Loss: 0.6003\n",
      "Epoch 10/10, Batch 1300, Loss: 0.6769\n",
      "Epoch 10/10, Batch 1400, Loss: 0.6067\n",
      "Epoch 10/10, Batch 1500, Loss: 0.5879\n",
      "Epoch 10/10, Batch 1600, Loss: 0.6584\n",
      "Epoch 10/10, Batch 1700, Loss: 0.5854\n",
      "Epoch 10/10, Batch 1800, Loss: 0.6466\n",
      "Epoch 10/10, Batch 1900, Loss: 0.5355\n",
      "Epoch 10/10, Batch 2000, Loss: 0.6694\n",
      "Epoch 10/10, Batch 2100, Loss: 0.6521\n",
      "Epoch 10/10, Batch 2200, Loss: 0.6708\n",
      "Epoch 10/10, Batch 2300, Loss: 0.6509\n",
      "Epoch 10/10, Batch 2400, Loss: 0.6394\n",
      "Epoch 10/10, Batch 2500, Loss: 0.6827\n",
      "Epoch 10/10, Batch 2600, Loss: 0.6385\n",
      "Epoch 10/10, Batch 2700, Loss: 0.6388\n",
      "Epoch 10/10, Batch 2800, Loss: 0.6026\n",
      "Epoch 10/10, Batch 2900, Loss: 0.6059\n",
      "Epoch 10/10, Batch 3000, Loss: 0.6339\n",
      "Epoch 10/10, Batch 3100, Loss: 0.6887\n",
      "Epoch 10/10, Batch 3200, Loss: 0.5797\n",
      "Epoch 10/10, Batch 3300, Loss: 0.6523\n",
      "Epoch 10/10, Batch 3400, Loss: 0.5991\n",
      "Epoch 10/10, Batch 3500, Loss: 0.6410\n",
      "Epoch 10/10, Batch 3600, Loss: 0.6457\n",
      "Epoch 10/10, Average Loss: 0.6296\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

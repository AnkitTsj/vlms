{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vision Transformer (ViT) Architecture Demonstration\n",
    "\n",
    "This Jupyter Notebook demonstrates the architectural correctness of the Vision Transformer (ViT) implementation. We will:\n",
    "\n",
    "1.  **Define the ViT model components** (Patch Embedding, Positional Encoding, Encoder Block, Classification Head, and the full ViT model).\n",
    "2.  **Instantiate the ViT model** with example parameters.\n",
    "3.  **Create a dummy input image tensor**.\n",
    "4.  **Perform a forward pass** through the model, step-by-step, and print the **output shape at each stage** to verify that it aligns with the expected ViT architecture.\n",
    "\n",
    "This notebook focuses on **architectural verification** and does not include training or performance evaluation."
   ],
   "id": "e52644747cbef369"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T12:44:29.491008Z",
     "start_time": "2025-02-18T12:44:27.226670Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from model import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:44:49.293223Z",
     "start_time": "2025-02-18T12:44:49.282360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Patch_Embedding_Projection(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patch_former = nn.Unfold(kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "        self.lin_proj = nn.Linear(in_features=patch_size ** 2 * self.in_channels, out_features=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_former(x)\n",
    "        x = patches.transpose(1, 2)\n",
    "        return self.lin_proj(x)"
   ],
   "id": "2923d63240e9417a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:44:56.793381Z",
     "start_time": "2025-02-18T12:44:56.785632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class POS_ENC(nn.Module):\n",
    "    def __init__(self, seq_len, enc_dim):\n",
    "        super(POS_ENC, self).__init__()\n",
    "        self.enc_dim = enc_dim\n",
    "        self.pos_embeddings = nn.Parameter(torch.randn(1, seq_len, enc_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embeddings"
   ],
   "id": "749f84ba93c1fd1b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:06.991605Z",
     "start_time": "2025-02-18T12:45:06.979984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, enc_dim, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_dim = enc_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = enc_dim // num_heads\n",
    "\n",
    "        assert enc_dim % num_heads == 0, \"enc_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "\n",
    "        self.wq = nn.Linear(in_features=enc_dim, out_features=enc_dim)\n",
    "        self.wk = nn.Linear(in_features=enc_dim, out_features=enc_dim)\n",
    "        self.wv = nn.Linear(in_features=enc_dim, out_features=enc_dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=enc_dim, out_features=enc_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=enc_dim*4, out_features=enc_dim))\n",
    "        self.proj_o = nn.Linear(in_features=enc_dim, out_features=enc_dim)\n",
    "\n",
    "    def attention(self, x):\n",
    "        batch_size, seq_len, enc_dim = x.shape\n",
    "\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        k_q = torch.matmul(q, k.transpose(-2, -1))\n",
    "        s_kq = self.softmax(k_q / (self.head_dim**0.5))\n",
    "        attn_output_heads = torch.matmul(s_kq, v)\n",
    "\n",
    "        attn_output_concat = attn_output_heads.transpose(1, 2).contiguous().view(batch_size, seq_len, enc_dim)\n",
    "        attn_output_proj = self.proj_o(attn_output_concat)\n",
    "\n",
    "        return attn_output_proj\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_residual = x\n",
    "        x =  self.norm1(x)\n",
    "        attn_output = self.attention(x)\n",
    "        x = attn_output + x_residual\n",
    "\n",
    "        x_residual_mlp = x\n",
    "        x = self.norm2(x)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = mlp_output + x_residual_mlp\n",
    "        return x"
   ],
   "id": "ef9377b54ec8b0c0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:12.206201Z",
     "start_time": "2025-02-18T12:45:12.200360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ViTClassifierHead(nn.Module):\n",
    "    def __init__(self, enc_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=enc_dim, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        class_token = x[:, 0, :]\n",
    "        logits = self.fc(class_token)\n",
    "        return logits"
   ],
   "id": "88736b59add84710",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:17.323160Z",
     "start_time": "2025-02-18T12:45:17.311589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, num_classes, embedding_dim, depth, num_heads):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.seq_len = self.num_patches + 1\n",
    "\n",
    "        self.patch_embed_proj = Patch_Embedding_Projection(patch_size=patch_size, in_channels=in_channels, embedding_dim=embedding_dim)\n",
    "        self.pos_embedding = POS_ENC(seq_len=self.seq_len,enc_dim=self.embedding_dim)\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        self.encoder_layers = nn.Sequential(*[Encoder(enc_dim=embedding_dim, num_heads=num_heads) for _ in range(depth)])\n",
    "        self.norm_head = nn.LayerNorm(embedding_dim)\n",
    "        self.classifier_head = ViTClassifierHead(enc_dim=embedding_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        patch_embeddings = self.patch_embed_proj(x)\n",
    "\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((class_token, patch_embeddings), dim=1)\n",
    "        embeddings_with_pos = self.pos_embedding(embeddings)\n",
    "\n",
    "        encoded_sequence = self.encoder_layers(embeddings_with_pos)\n",
    "        pooled = self.norm_head(encoded_sequence)\n",
    "        logits = self.classifier_head(pooled)\n",
    "        return logits"
   ],
   "id": "2344795c7b59685e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instantiate the ViT Model\n",
    "\n",
    "Here, we instantiate the `ViTForImageClassification` model with some example parameters. These parameters are based on the ViT-Base configuration but can be adjusted.\n",
    "\n",
    "**Parameters:**\n",
    "*   `image_size`: Input image size (e.g., 224x224).\n",
    "*   `patch_size`: Size of image patches (e.g., 16x16).\n",
    "*   `in_channels`: Number of input image channels (e.g., 3 for RGB).\n",
    "*   `num_classes`: Number of classes for classification (e.g., 1000 for ImageNet).\n",
    "*   `embedding_dim`: Embedding dimension for patches and tokens (e.g., 768).\n",
    "*   `depth`: Number of Transformer Encoder layers (e.g., 12).\n",
    "*   `num_heads`: Number of attention heads in Multi-Head Self-Attention (e.g., 12)."
   ],
   "id": "5df6f0a5edc4c4d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:32.415844Z",
     "start_time": "2025-02-18T12:45:32.118376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example parameters (ViT-Base like)\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "in_channels = 3\n",
    "num_classes = 1000\n",
    "embedding_dim = 768\n",
    "depth = 12\n",
    "num_heads = 12\n",
    "\n",
    "# Instantiate the ViT model\n",
    "model = ViTForImageClassification(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    in_channels=in_channels,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=embedding_dim,\n",
    "    depth=depth,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "print(\"ViT Model Instantiated!\")"
   ],
   "id": "7ce4051ccbcc9b67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Model Instantiated!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a Dummy Input Image\n",
    "\n",
    "We create a dummy input image tensor to simulate a batch of images passing through the network. The shape should be `(batch_size, in_channels, image_size, image_size)`."
   ],
   "id": "a83b08c23689d227"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:47.070021Z",
     "start_time": "2025-02-18T12:45:47.063428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 2 # Example batch size\n",
    "dummy_input = torch.randn(batch_size, in_channels, image_size, image_size)\n",
    "\n",
    "print(\"Dummy Input Image Shape:\", dummy_input.shape)"
   ],
   "id": "c6af2db416f51104",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Input Image Shape: torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Patch Embedding and Projection\n",
    "\n",
    "We pass the dummy input image through the `Patch_Embedding_Projection` layer. This layer should:\n",
    "*   Divide the image into patches of size `patch_size` x `patch_size`.\n",
    "*   Flatten each patch.\n",
    "*   Project the flattened patches to `embedding_dim`.\n",
    "\n",
    "The expected output shape is `(batch_size, num_patches, embedding_dim)`, where `num_patches = (image_size / patch_size) ** 2`."
   ],
   "id": "226eb313803fe620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:45:59.556412Z",
     "start_time": "2025-02-18T12:45:59.539281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "patch_embeddings = model.patch_embed_proj(dummy_input)\n",
    "\n",
    "print(\"Patch Embeddings Shape:\", patch_embeddings.shape)\n",
    "expected_num_patches = (image_size // patch_size) ** 2\n",
    "assert patch_embeddings.shape == (batch_size, expected_num_patches, embedding_dim), \\\n",
    "       f\"Patch Embeddings shape is incorrect. Expected {(batch_size, expected_num_patches, embedding_dim)}, but got {patch_embeddings.shape}\""
   ],
   "id": "dde18d85a0b7c0ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embeddings Shape: torch.Size([2, 196, 768])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding\n",
    "\n",
    "We add positional embeddings to the patch embeddings using the `POS_ENC` layer. This layer adds learnable positional embeddings to each position in the sequence.\n",
    "\n",
    "The shape should remain the same: `(batch_size, num_patches + 1, embedding_dim)` because we prepend the class token later, but for now we check the shape after positional encoding is applied to patch embeddings *before* the class token is prepended in the full model's forward pass."
   ],
   "id": "cfb9b10063cc39ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:46:11.748774Z",
     "start_time": "2025-02-18T12:46:11.741385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create class token and prepend, then apply positional encoding (simulating inside ViTForImageClassification forward)\n",
    "class_token = model.class_token.expand(batch_size, -1, -1)\n",
    "embeddings_with_class_token = torch.cat((class_token, patch_embeddings), dim=1)\n",
    "embeddings_with_pos = model.pos_embedding(embeddings_with_class_token)\n",
    "\n",
    "print(\"Embeddings with Positional Encoding Shape:\", embeddings_with_pos.shape)\n",
    "expected_seq_len = expected_num_patches + 1 # +1 for class token\n",
    "assert embeddings_with_pos.shape == (batch_size, expected_seq_len, embedding_dim), \\\n",
    "       f\"Embeddings with Positional Encoding shape is incorrect. Expected {(batch_size, expected_seq_len, embedding_dim)}, but got {embeddings_with_pos.shape}\""
   ],
   "id": "bc2be5e9fe8e92a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with Positional Encoding Shape: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer Encoder Layers\n",
    "\n",
    "We pass the embeddings with positional encoding through the stacked Transformer Encoder layers (`model.encoder_layers`). Each Encoder layer consists of Multi-Head Self-Attention and MLP blocks.\n",
    "\n",
    "The shape should remain the same throughout the Encoder layers: `(batch_size, num_patches + 1, embedding_dim)`."
   ],
   "id": "a84eb33f8e16e8be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:46:22.520332Z",
     "start_time": "2025-02-18T12:46:22.188486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_sequence = model.encoder_layers(embeddings_with_pos)\n",
    "\n",
    "print(\"Output Shape after Transformer Encoder Layers:\", encoded_sequence.shape)\n",
    "assert encoded_sequence.shape == (batch_size, expected_seq_len, embedding_dim), \\\n",
    "       f\"Encoder Output shape is incorrect. Expected {(batch_size, expected_seq_len, embedding_dim)}, but got {encoded_sequence.shape}\""
   ],
   "id": "9e1a92ffc8dfe37c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape after Transformer Encoder Layers: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification Head\n",
    "\n",
    "Finally, we pass the output from the Transformer Encoder through the `ViTClassifierHead`. This head extracts the `[class]` token representation and projects it to class logits.\n",
    "\n",
    "The expected output shape is `(batch_size, num_classes)`, representing the logits for each class."
   ],
   "id": "47860c404a96d10f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:46:33.514891Z",
     "start_time": "2025-02-18T12:46:33.506835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_logits = model.classifier_head(encoded_sequence)\n",
    "\n",
    "print(\"Output Logits Shape from Classification Head:\", output_logits.shape)\n",
    "assert output_logits.shape == (batch_size, num_classes), \\\n",
    "       f\"Classification Head output shape is incorrect. Expected {(batch_size, num_classes)}, but got {output_logits.shape}\""
   ],
   "id": "c43642a27b0f0067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Logits Shape from Classification Head: torch.Size([2, 1000])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full Forward Pass Verification\n",
    "\n",
    "To further confirm, let's perform a full forward pass through the entire `ViTForImageClassification` model in one step and check the final output logits shape."
   ],
   "id": "ca9a6933c6be4f58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T12:46:46.415122Z",
     "start_time": "2025-02-18T12:46:46.117003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_output_logits = model(dummy_input)\n",
    "\n",
    "print(\"Output Logits Shape from Full Forward Pass:\", full_output_logits.shape)\n",
    "assert full_output_logits.shape == (batch_size, num_classes), \\\n",
    "       f\"Full Forward Pass output shape is incorrect. Expected {(batch_size, num_classes)}, but got {full_output_logits.shape}\""
   ],
   "id": "f525da46694c4502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Logits Shape from Full Forward Pass: torch.Size([2, 1000])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "The notebook successfully demonstrated that the implemented Vision Transformer (ViT) code works according to the expected architecture.\n",
    "\n",
    "**Verification Points:**\n",
    "*   **Patch Embedding:** The `Patch_Embedding_Projection` layer correctly divides the image into patches and projects them to the embedding dimension.\n",
    "*   **Positional Encoding:** Positional embeddings are correctly added to the patch embeddings.\n",
    "*   **Transformer Encoder:** The Transformer Encoder layers maintain the sequence length and embedding dimension.\n",
    "*   **Classification Head:** The `ViTClassifierHead` correctly extracts the `[class]` token representation and projects it to the final logits of shape `(batch_size, num_classes)`.\n",
    "\n",
    "**Next Steps:**\n",
    "*   Train the ViT model on an image classification dataset (e.g., CIFAR-10, ImageNet).\n",
    "*   Evaluate the model's performance on validation and test sets.\n",
    "*   Experiment with different ViT configurations (depth, embedding dimension, number of heads, patch size).\n",
    "\n",
    "This notebook provides a solid foundation for further experimentation and development with Vision Transformers."
   ],
   "id": "4f1abae4e8a6afa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31134c08c46c7be0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
